%\section{Implementation}\label{Sec: implementation}
\section{From Theory into Practice}\label{Sec: implementation}
Generating provenance-based citations for aggregate queries and views relies on ideas from query rewriting using views as well as provenance. However, bringing those ideas from theory into practice raises several engineering challenges, which must be overcome to provide a solution with acceptable time performance.  We discuss those challenges in this section, starting with a discussion of the algorithmic complexity of \provalg, before discussing implementation details and optimizations used in the context of an example.
\eat{then discussing the optimizations used, and ending with an example illustrating the implementation and several of the optimizations.}


\subsection{Algorithmic complexity}
In what follows, we assume that the underlying database system is \textit{provenance-enabled}, i.e. we do not consider the cost of carrying provenance through queries and generating the how-provenance of final results.  Rather, we focus on the cost of \textit{using} provenance to generate fine-grained citations.

An overview of our implementation is shown in Algorithm \ref{covering_sets_calculation}.  The algorithm consists of three steps: 1) {\em Preprocessing}, 2) {\em Reasoning about valid view mappings}, and 3) {\em Covering sets calculation}. 
%\scream{maybe better to remove the following sentence} The complexity of generating fine-grained citations is dominated by the cost of calculating the covering sets for each tuple. 

The major overhead of the {\em Preprocessing} step is loading the query provenance into memory, which is determined by the underlying provenance-enabled database. \eat{proportional to the total number of how-provenance monomials in the query instance (denoted as $N_{pq}$).}

In the {\em Reasoning about valid view mappings} step, 
%suppose there are $m$ view mappings from a set of views $\mathcal{V}$ to query $Q$, 
the time to check the validity of a view mapping includes: 1) the time to retrieve the view provenance from the database, which is proportional to the total number of how-provenance monomials in the view instance on average (denoted as $N_{pv}$); and 2) the time to compare the view provenance and query provenance in memory, which is proportional to the total number of how-provenance monomials in the query ($N_{pq}$). As a consequence, if there are $m$ view mappings the time complexity for this step is $O(m*N_{pq}) + O(m*N_{pv})$. Suppose $k$ is the upper bound for the number of relational subgoals in the query or view body and the largest relation in the database has $n$ tuples, then $O(N_{pq}) = O(N_{pv}) = O(n^k)$ and thus the time complexity becomes $O(m*n^k)$.  Our experiments with realistic queries in Section \ref{sec:experiments}\eat{~\ref{ssec: realistic}}, however, show that in practice the performance is still acceptable since both $N_{pq}$ and $N_{pv}$ are not very large (usually no more than 1 million).\eat{$n$ and $k$ are both relatively small: $n$ is on the order of 10k tuples and $k$ less than 4.}


The time for the {\em Covering sets calculation} step depends on the {\em policies} on how to convert {\em covering sets} into formatted citations
% used for joint ($*$), alternate ($+^R$) and aggregated use ($Agg$) 
-- see~\cite{wu2018data}.  
Since in the worst case the number of possible coverings sets may be exponential in $m$, if $m$ is large and all covering sets are used in a policy this can be very costly.  
However, in practice $m$ is small (e.g. 1 or 2) since views associate \textit{different parts} of the database with citations and queries have a small number of subgoals. Performance is therefore acceptable even when all covering sets are used in a policy, as shown in Section~\ref{sec:experiments}.  Furthermore, in practice we believe that a ``minimal cost'' policy will be used to generate concise citations, in which case covering sets are pruned,  resulting in a cost which is linear in $m$.


\begin{algorithm}[h!] 
\footnotesize
 \SetKwInOut{Input}{Input}
 \SetKwInOut{Output}{Output}
 \Input{a set of views: $\mathcal{V} = \{V_1, V_2,...,V_k\}$, user query: $Q$, a Database instance $D$}

 \Output{Covering sets for every query tuple in $Q(D)$}

{\em Preprocessing step}: Return a set of all possible view mappings $\mathcal{M}$ and the provenance of $Q$

{\em Reasoning valid view mapping step}: Retrieve provenance of every view. For each query tuple $t$, determine valid view mappings by comparing the provenance of $Q$ and the provenance of $V$

{\em Covering sets calculation step}: Calculate covering sets by combining valid view mappings for each query tuple.
%  \Return $\mathcal{M}$
 \caption{Overview of PBA}
 \label{covering_sets_calculation}
 \end{algorithm}

\eat{\subsection{Optimizations}
Our worst case complexity analysis shows that it is potentially quite expensive to generate fine-grained citations, and therefore challenging to do with acceptable performance.  We therefore use a number of optimizations for the second and third steps, since the time in the first step is determined by the query time to the underlying provenance-enabled database and out of our control.}

%pre_processing step
%executing $Q$ over a provenance-middleware system called ``gprom''\cite{arab2018gprom}. Note that, in terms of provenance information for $Q(D)$, ``gprom'' represents the how-provenance monomials in the form of corresponding base relation tuples;

%Checking conjunctive parts
\eat{{\bf Reasoning about valid view mappings.} We use two optimizations in this step.  First, all {\em schema-level conditions} (see Section \ref{sec: model}) are applied to remove invalid view mappings early.}%, as done in~\cite{wu2018data}.

\eat{Second, the validity of a view mapping $M$ involving a \textit{conjunctive} view $V$ can be determined by reasoning about the \textit{satisfiability of predicates} in $V$ under $M$; there is no need to retrieve the provenance of  $V$.  The provenance of each tuple in the query result, which is expressed in terms of the base relations, is 
% create an instance which is 
evaluated against the view \textit{predicates} to determine whether or not the tuple would appear in the view.}

\eat{For \textit{aggregate} views, two alternative strategies can be used. One is to pre-compute their how-provenance ({\em eager strategy}) while the other one is to retrieve their  how-provenance on the fly ({\em lazy strategy}). The trade-offs between the two options will be discussed in Section~\ref{sec:experiments}. }

% As mentioned in Section \ref{sec: model},  the where-provenance is not necessary and was only introduced to clarify the ideas.  It is therefore not generated in the implementation.

% Some optimization strategies are applied in this step, which includes 1) the head of the view $V$ is extended by applying one built-in function called ``array\_agg'' in Postgresql to speed up the process of retrieving how-provenance monomials from the database; 2) the where-provenance is not necessary here since the combinations of how-provenance monomials and view mappings are enough (proof is omitted).
% In this step, the validity of view mapping $M$ which maps a view $V$ to $Q$ is determined for each query tuple $t_q$. For local and global predicates of $V$, it is not hard to check their satisfiability under $M$ by evaluating those predicates using the the base relation tuples associated with query instance. 

% Further steps are needed iff aggregations are involved in $V$, i.e. reasoning by comparing the provenance of $V$ and $Q$, which starts by retrieving the provenance of $V$ into memory. Two alternative strategies are available for this. One is to pre-compute materialized views along with the how-provenance monomials while the other one is to execute the view queries and grab the associated how-provenance monomials from the base relations on the fly. In order to generate how-provenance monomials for $V$ properly, the head of $V$ is extended to include another aggregate term, which returns all the provenance monomials and composes them as a collection for each group of values. The trade-offs between the two options will be discussed in Experiment section. 

% Then the conditions proposed in Section \ref{sec:model} are applied to compare the provenance of $V$ and $Q$. Note that the where-provenance is not necessary here since the combinations of how-provenance monomials and view mappings are enough (proof is omitted). \eat{For a given query tuple $t_q$, a set of candidate view tuples $\mathcal{T}_v$ are determined, which should share the same grouping values with $t_q$ and include at least one how-provenance monomial of $t_q$ under a view mapping $M$. The validity of $M$ should depend on whether some multiset of tuples from the set $\mathcal{T}_v$ include the same how-provenance monomials with $t_q$.}

\eat{{\bf Covering sets calculation step.}
Three strategies are applied to speed up the covering sets calculation: 1) representing coverings sets using bit arrays; 2) applying clustering algorithms to avoid an explosion of intermediate results; and 3) query tuples associated with the same set of valid view mappings are grouped together to avoid repetitive computations of covering sets within one group. These optimizations result in orders of magnitude speed-up.}  %(see \cite{techreport} for details).
% Once valid view mappings are determined for each query tuple $t_q$, covering sets of each $t_q$ are computed by applying an optimization strategy which is similar to in \cite{wu2018data}, i.e. query tuples associated with the same set of valid view mappings are grouped together so that the computation of covering sets can be simply done once for each group.

% \begin{table}[htp]
% \centering
% \small
% \caption{The instance of relation $Gene$}\label{Instance of Gene}
% \begin{tabular}[t]{c|c|c|c|c|c|c|c|} \hhline{~-------}
% &GID&&Name&&Type&&\\ \hhline{~-------}
% $t_{g1}$&1&$g_1$&TF&$g_2$&TEC&$g_3$&$G_1$\\ \hhline{~-------}
% $t_{g2}$&2&$g_5$&FH&$g_6$&rRNA&$g_7$&$G_2$\\ \hhline{~-------}
% \end{tabular}
% \bigskip
% \caption{The instance of relation $Transcript$}\label{Instance of Transcript}
% \begin{tabular}[t]{c|c|c|c|c|c|c|c|c|c|} \hhline{~---------}
% &TID&&Name&&Type&&GID&&\\ \hhline{~---------}
% $t_{t1}$&1&$t_1$&MB-203&$t_2$&TEC&$t_3$&1&$t_4$&$T_1$\\ \hhline{~---------}
% $t_{t2}$&2&$t_5$&PC-203&$t_6$&rRNA&$t_7$&2&$t_8$&$T_2$\\ \hhline{~---------}
% $t_{t3}$&4&$t_9$&HP-218&$t_{10}$&rRNA&$t_{11}$&2&$t_{12}$&$T_3$\\ \hhline{~---------}
% $t_{t4}$&5&$t_{13}$&GK-207&$t_{14}$&rRNA&$t_{15}$&2&$t_{16}$&$T_4$\\ \hhline{~---------}
% \end{tabular}
% % \bigskip
% % \caption{The instance of relation $Exon$}\label{Instance of Exon}
% % \begin{tabular}[t]{c|c|c|c|c|c|c|c|} \hhline{~-------}
% % &EID&&Level&&TID&&\\ \hhline{~-------}
% % $t_{e1}$&1&$e_1$&1&$e_2$&1&$e_3$&$E_1$\\ \hhline{~-------}
% % $t_{e2}$&2&$e_4$&3&$e_5$&2&$e_6$&$E_2$\\ \hhline{~-------}
% % $t_{e3}$&3&$e_7$&3&$e_{8}$&2&$e_{9}$&$E_3$\\ \hhline{~-------}
% % $t_{e4}$&4&$e_{10}$&2&$e_{11}$&4&$e_{12}$&$E_4$\\ \hhline{~-------}
% % $t_{e5}$&5&$e_{13}$&3&$e_{14}$&5&$e_{15}$&$E_5$\\ \hhline{~-------}
% % $t_{e6}$&6&$e_{16}$&2&$e_{17}$&5&$e_{18}$&$E_6$\\ \hhline{~-------}
% % $t_{e7}$&7&$e_{19}$&2&$e_{20}$&5&$e_{21}$&$E_7$\\ \hhline{~-------}
% % \end{tabular}
% \end{table}

\subsection{Optimization and implementation (via an example)}
Our worst case complexity analysis shows that if naively implemented, it might be quite expensive to generate fine-grained citations, and therefore challenging to do with acceptable performance.  We therefore use a number of optimizations, which are discussed below using an example which also illustrates how \provalg\ is implemented. 
\begin{example}
Given the views $V_1-V_6$ defined in Section \ref{subsec:running example}, suppose the query is as follows:
% \begin{tabbing}
% $Q(T, N, COUNT(E), MAX(L)):-Exon(E, L, T'), E <= 6$\\
% \tab\tab\tab\tab$Transcript(T, N, Ty, G), T = T'$
% % \tab\tab\tab$$
% \end{tabbing}
\begin{tabbing}
$Q(G, COUNT(T), MAX(L), COUNT(E)):-Exon(E, L, T')$\\
\tab\tab$,Transcript(T, N, Ty, G), T = T', E <= 6$
% \tab\tab\tab$$
\end{tabbing}
%How to do pre-processing step
In the {\em pre-processing} step,  the provenance of the query is retrieved. \eat{The query time is determined by the underlying provenance-enabled database, which is out of our control and thus not optimized.}Using the instances of Exon, Gene and Transcript shown in Tables \ref{Instance of Exon}-\ref{Instance of Transcript}, the instance of $Q$ along with the how-provenance polynomials is shown in Table \ref{Instance of Q1}. 
% Note that the where-provenance is not computed since it is not needed. 

\begin{table}[htp]
\centering
\small
\caption{$Q(D)$ with how-provenance polynomials}\label{Instance of Q1}
% \begin{tabular}[t]{c|c|c|c|c||b|} \hhline{~-----}
% &T&N&COUNT(E)&MAX(L)&prov\\ \hhline{~-----}
% $t_{q1}$&1&MB-203&1&1&$e_1*r_1$\\ \hhline{~-----}
% $t_{q2}$&2&PC-203&2&3&$e_2*r_2 + e_3*r_2$\\ \hhline{~-----}
% $t_{q3}$&4&HP-218&1&2&$e_4*r_3$\\ \hhline{~-----}
% $t_{q4}$&5&GK-207&2&3&$e_5*r_4 + e_6*r_4$\\ \hhline{~-----}
% \end{tabular}
\hspace*{-0.5cm}
\begin{tabular}[t]{c|>{\centering\arraybackslash}p{0.15cm}|>{\centering\arraybackslash}p{1.5cm}|c|c||b|} \hhline{~-----}
&G&COUNT(T)&MAX(L)&COUNT(E)&prov\\ \hhline{~-----}
$t_{q1}$&1&1&1&1&$e_1*r_1$\\ \hhline{~-----}
$t_{q2}$&2&2&3&2&$e_2*r_2 + e_3*r_2$\\ \hhline{~-----}
$t_{q3}$&3&3&3&3&\makecell{$e_4*r_3 + e_5*r_4 \\+ e_6*r_4$}\\ \hhline{~-----}
\end{tabular}
\bigskip
\caption{View mappings from $V_3-V_6$ to $Q$}\label{Table: view mapping}
\begin{tabular}[t]{|c|c|c|} \hline
\makecell{view \\mapping}&\makecell{aggregate \\ terms covered}&\makecell{relational  subgoals \\covered}\\ \hline
\makecell{$M_3$\\ (1000)}&\makecell{COUNT(T),MAX(L), \\COUNT(E) (111)}&\makecell{Transcript(T, N, Ty, G),\\Exon(E, L, T') (11)}\\ \hline
\makecell{$M_4$\\ (0100)}&\makecell{COUNT(T) (100)}&\makecell{Transcript(T, N, Ty, G),\\Exon(E, L, T') (11)}\\ \hline
\makecell{$M_5$\\ (0010)}&\makecell{MAX(L), COUNT(E)\\ (011)}&\makecell{Transcript(T, N, Ty, G),\\Exon(E, L, T') (11)}\\ \hline
\makecell{$M_6$\\ (0001)}&\makecell{COUNT(T) (100)}&\makecell{Transcript(T, N, Ty, G)\\ (01)}\\ \hline
\end{tabular}
\end{table}

Next, all possible view mappings are constructed. We can find three view mappings, $M_3$, $M_4$ and $M_5$, under which the relational subgoals of $V_3$, $V_4$ and $V_5$ are mapped to subgoals of $Q$ with the same name, respectively. Note that all the {\em schema-level conditions} are independent from individual query tuples, which can be applied to remove invalid view mappings early. In this example, $M_3$, $M_4$ and $M_5$ are all satisfied, since under each mapping all the grouping variables and at least one aggregate term of $Q$ are {\em covered}.
% early. since $V4$ and $V5$ are almost the same as $Q$ (except the differences in one local predicate).
%while other views either miss grouping attributes (such as $V3$ and $V1$) or cannot provide necessary attributes to compute the statistics that are needed for $Q_1$ (such as $V2$).

In the {\em Reasoning about valid view mappings} step, since $V_3$ is a \textit{conjunctive view}, the validity of its view mapping $M_3$ can be determined by reasoning about the \textit{satisfiability of predicates} in $V_3$ under $M_3$ without retrieving the provenance of $V_3$. This is because the base relation tuple $t$ for each provenance token appearing in the query can be retrieved from the database beforehand to evaluate the view \textit{predicates} to determine whether or not the tuple would appear in the view. For example, the validity of $M_3$ can be checked simply by examining the predicates of $V_3$. Since the first predicate, $T = T'$, in $V_3$ is the same as that in $Q$, every tuple in the query instance must satisfy it. The second predicate, $E >= 4$, is only related to relation Exon and the how-provenance tokens $e_1-e_6$ in $Q(D)$. Looking at Table \ref{Instance of Exon}, only the tuples with how-provenance token $e_4-e_6$ satisfy $E >= 4$. Thus $M_3$ is only valid for $t_{{q}3}$ and $t_{{q}4}$, whose how-provenance polynomials only include $e_4-e_6$ without any other how-provenance tokens from Exon. 

\begin{table}
\centering
\small
\caption{$V_4(D)$ with how-provenance polynomials}\label{Instance of V4}
% \begin{tabular}[t]{c|c|c|c||b|} \hhline{~----}
% &T1&N1&COUNT(E)&prov\\ \hhline{~----}
% $t_{v_41}$&4&HP-218&1&$e_4*r_3$\\ \hhline{~----}
% $t_{v_42}$&5&GK-207&2&$e_6*r_4 + e_7*r_4$\\ \hhline{~----}
% \end{tabular}
\begin{tabular}[t]{c|c|c||b|} \hhline{~---}
&G1&COUNT(T1)&prov\\ \hhline{~---}
$t_{v_41}$&1&1&$e_1*r_1$\\ \hhline{~---}
$t_{v_42}$&3&3&\makecell{$e_4*r_3 + e_6*r_4 + e_7*r_4$}\\ \hhline{~---}
\end{tabular}
\bigskip
\caption{$V_5(D)$ with how-provenance polynomials}\label{Instance of V5}
% \begin{tabular}[t]{c|c|c|c||b|} \hhline{~----}
% &T1&N1&MAX(L)&prov\\ \hhline{~----}
% $t_{v_51}$&1&MB-203&1&$e_1*r_1$\\ \hhline{~----}
% $t_{v_52}$&2&PC-203&3&$e_2*r_2 + e_3*r_2$\\ \hhline{~----}
% $t_{v_53}$&4&HP-218&2&$e_4*r_3$\\ \hhline{~----}
% $t_{v_54}$&5&GK-207&3&$e_5*r_4 + e_6*r_4 + e_7*r_4$\\ \hhline{~----}
% \end{tabular}
\begin{tabular}[t]{c|c|c|c||b|} \hhline{~----}
&G1&MAX(L)&COUNT(E)&prov\\ \hhline{~----}
$t_{v_51}$&1&1&1&$e_1*r_1$\\ \hhline{~----}
$t_{v_52}$&2&3&2&$e_2*r_2 + e_3*r_2$\\ \hhline{~----}
$t_{v_53}$&3&3&4&\makecell{$e_4*r_3 + e_5*r_4\\ + e_6*r_4 + e_7*r_4$}\\ \hhline{~----}
\end{tabular}
\bigskip
\caption{Instance of relation $V_6(D)$ with provenance}\label{Instance of V6}
\begin{tabular}[t]{c|c|c||b|} \hhline{~---}
&G&COUNT(T)&prov\\ \hhline{~---}
$t_{v_61}$&1&1&$r_1$\\ \hhline{~---}
$t_{v_62}$&2&1&$r_2$\\ \hhline{~---}
$t_{v_63}$&3&2&$r_3 + r_4$\\ \hhline{~---}
\end{tabular}
\end{table}


% \begin{table}[htp]
% \centering
% \small

% \end{table}

% \begin{table}[htp]
% \centering
% \small
% \caption{The instance of relation $Q_1$ along with how-provenance polynomials}\label{Instance of Q1}
% \begin{tabular}[t]{c|c|c|c|c|c|c|c|} \hhline{~-------}
% &T1&&N1&&COUNT(E)&MAX(L)&\\ \hhline{~-------}
% $t_{q_11}$&1&$\{t_1\}$&MB-203&$\{t_2\}$&1&1&$E_1*T_1$\\ \hhline{~-------}
% $t_{q_12}$&2&$\{t_5\}$&PC-203&$\{t_6\}$&2&3&$E_2*T_2 + E_3*T_2$\\ \hhline{~-------}
% $t_{q_13}$&4&$\{t_9\}$&HP-218&$\{t_6\}$&1&2&$E_4*T_3$\\ \hhline{~-------}
% $t_{q_14}$&5&$\{t_{13}\}$&GK-207&$\{t_6\}$&2&3&$E_5*T_4 + E_6*T_4$\\ \hhline{~-------}
% \end{tabular}
% \end{table}

% \begin{table}[htp]
% \centering
% \small

% \end{table}

% \begin{table}[htp]
% \centering
% \small
% \caption{The instance of relation $V5$}\label{Instance of V5}
% \begin{tabular}[t]{c|c|c|c|c|} \hhline{~----}
% &T1&N1&MAX(L)&\\ \hhline{~----}
% $t_{v_51}$&1&MB-203&1&$\{\{E_1,T_1\}\}$\\ \hhline{~----}
% $t_{v_52}$&2&PC-203&3&$\{\{E_2,T_2\}, \{E_3,T_2\}$\\ \hhline{~----}
% $t_{v_53}$&4&HP-218&2&$\{\{E_4,T_3\}\}$\\ \hhline{~----}
% $t_{v_54}$&5&GK-207&3&$\{\{E_5,T_4\}, \{E_6,T_4\}, \{E_7,T_4\}\}$\\ \hhline{~----}
% \end{tabular}
% \end{table}

% \begin{table}[htp]
% \centering
% \small

% \end{table}

In contrast, since both $V_4$, $V_5$ and $V_6$ are aggregate views, we need to compare their how-provenance expressions with the how-provenance of the query to check the \textit{tuple-level conditions}. A naive solution for it is simply to scan the entire query provenance (at least once) for {\em every view mapping} to attempt to map every view provenance to the query provenance, which is expensive especially when $N_{pv}$ and $N_{pq}$ are large. Hashmap (mapping the grouping attribute values to the how-provenance monomial set in the query) seems to be helpful to speed up the searching, which, however, still needs to scan the entire query provenance for every view mapping for initialization since the portion of how-provenance monomials in the query {\em involved} in the view mappings may be varied across different view mappings. For example, the view mapping $M_6$ only covers the subgoal $Transcript$ in the query body and thus only the provenance token $r_x (x=1,2,...)$ should be {\em involved} in $M_6$. However, both the token $r_x$ and $e_y$ are {\em involved} in $M_4$ and $M_5$. Such difference can lead to totally different HashMaps and thus inevitable repetitive scans over the entire query provenance for every view mapping in the worst case.

In order to avoid multiple scans over query provenance, we built an index $I$ for each single token in the query provenance to indicate which query tuples (represented by grouping attribute values) and which provenance monomials it lies in. For example, for token $r_4$ in the query provenance, since it is in the second and third monomial in the query tuple $t_{q3}$, such {\em coordinate information} is thus stored in the index $I$ taking $r_4$ as the key. For a how-provenance monomial in the view, e.g. $e_4*r_3$ in $t_{v_42}$, to determine whether it can be mapped to some query provenance monomial, we can retrieve the {\em coordinate information} for $e_4$ and $r_3$ respectively (if any) and then perform intersection over the {\em coordinate information} of the two tokens to check whether they coexist in some how-provenance monomial in the query provenance. It turns out that the common position for $e_4$ and $r_3$ is the first monomial in the tuple $t_{q3}$. We can further optimize the intersection operations by representing the monomial coordinate with bit array ($i_{th}$ bit is 0/1 iff a token is/isn't in the $i_{th}$ monomial of a certain query tuple) and applying bit AND operations for speed-ups. It turns out that this strategy only requires one full scan over the query provenance for all the view mappings.



Since the provenance of the aggregate queries is necessary for reasoning, to further gain better performance, the view instance and the corresponding provenance information can be materialized in the database before the query comes. Two different strategies (with and without materialization step) are compared in Section \ref{sec:experiments}, which are called {\em eager strategy} and {\em lazy strategy} respectively.

% and which can be dealt with by two alternative strategies. One is to pre-compute their how-provenance ({\em eager strategy}) while the other one is to retrieve their how-provenance on the fly ({\em lazy strategy}). Their trade-offs will be discussed in Section~\ref{sec:experiments}.

Given the instances and provenance expressions of $V_4-V_6$ (presented in Tables \ref{Instance of V4}-\ref{Instance of V6}), after applying those tricks above, we find that view mapping $M_4$ is only valid for tuple $t_{q1}$ since it shares the same set of how-provenance monomials with view tuple $t_{v_41}$.
%some query tuples like $t_{q_11}$ and $t_{q_12}$ have no view tuples sharing the same grouping values with them while other tuples like $t_{q_14}$ do not share the same set of how-provenance monomials with any view tuple(s) although it has the same value on the group variables and aggregated term $COUNT(E)$ as $t_{v_42}$. 

Similarly, we can determine that $M_5$ ($M_6$ resp.) is valid for query tuples $t_{q1}$ - $t_{q2}$ ($t_{q1}$ - $t_{q3}$ resp.). Note that for tuple $t_{q3}$, although all of its how-provenance monomials exist in the view tuple $t_{v_54}$, it does not include
%base relation tuple with how-provenance monomial 
$e_7*r_4$ which is used to construct $t_{v_53}$, violating the {\em Tuple-level condition}.  Intuitively, since the value of aggregate term may come from this component of the monomial ($e_7*r_4$), $t_{v_53}$ should not provide citation information for $t_{q3}$.


Finally, valid view mappings are shown in Table \ref{Instance of Q1 with view mappings} along with the query instance, which are then used to compute covering sets for each query tuple in the {\em Covering sets calculation step}. 
\eat{Note that for tuple $t_{{q}3}$, there are two covering sets, $\{M_3\}$ and $\{M_4, M_5\}$;  
% both of these sets cover the aggregate terms in $Q_1$ and are combined using $*$ and $+^R$. % as Table \ref{Instance of Q1 with view mappings} shows. covers both the aggregate terms in $Q1$. 
other combinations of view mappings either {redundantly} or {non-maximally} cover the query's aggregate terms, and therefore are not valid.}
%. Note that, $t_{q_11}$ and $t_{q_12}$ share the same set of valid view mappings and thus covering sets can be simply computed once for the two tuples. 
\begin{table}[htp]
\centering
\small
\caption{$Q(D)$ with valid view mappings and covering sets (aggregate terms omitted)}\label{Instance of Q1 with view mappings}
% \begin{tabular}[t]{c|c|c|c|c|c|c|} \hhline{~------}
% &T1&N1&COUNT(E)&MAX(L)&valid view mappings&covering sets\\ \hhline{~------}
% $t_{q_11}$&1&MB-203&1&1&$M_5$&$M_5$\\ \hhline{~------}
% $t_{q_12}$&3&PC-203&2&3&$M_5$&$M_5$\\ \hhline{~------}
% $t_{q_13}$&2&HP-218&1&2&$M_3, M_4, M_5$&$M_3 +^R M_4*M_5$\\ \hhline{~------}
% $t_{q_14}$&3&GK-207&2&3&$M_3$&$M_3$\\ \hhline{~------}
% \end{tabular}
\begin{tabular}[t]{c|c||c|c|} \hhline{~---}
&G&valid view mappings&covering sets\\ \hhline{~---}
$t_{q1}$&1&$M_3, M_4, M_5, M_6$&\makecell{$\{\{M_3\}, \{M_4, M_5\}, \{M_5, M_6\}\}$}\\ \hhline{~---}
$t_{q2}$&2&$M_5, M_6$&$\{\{M_5, M_6\}\}$\\ \hhline{~---}
$t_{q3}$&3&$M_6$&$\{M_6\}$\\ \hhline{~---}
\end{tabular}
\end{table}
\end{example}

As we observed in \cite{wu2018data}, it is time-consuming to compute {\em all} covering sets. Compared to \cite{wu2018data}, two strategies are utilized for speed-ups, i.e. 1) representing coverings sets using bit arrays; 2) applying clustering algorithms to avoid an explosion of intermediate results, which can lead to order of magnitude performance gains (see Section \ref{sec:experiments}).

% \subsubsection{Introducing bit arrays}
The computation of covering sets involves merging valid view mappings and removing duplicates, which can be optimized with bit operations. For example, for $Q$, the aggregate term $COUNT(T)$, $MAX(L)$ and $COUNT(E)$ are covered by $\{M_3, M_4, M_6\}$ (denoted by $S_1$), $\{M_3, M_5\}$ (denoted by $S_2$) and $\{M_3, M_5\}$ (denoted by $S_3$) respectively. We can encode the view mapping $M_3-M_6$ as $\{0,1,2,3\}$, the aggregate terms ($COUNT(T)$, $MAX(L)$ and $COUNT(E)$) as $\{0, 1, 2\}$ and the relational subgoals ($Exon$ and $Transcript$) as $\{0, 1\}$ such that each view mapping and covering set can be represented by three bit arrays where $i_{th}$ bit is 1 (0 resp.) iff $i_{th}$ view mapping is included (missing resp.) or $i_{th}$ aggregate terms or relational subgoal is covered (not covered resp.). For instance, $M_4$ is $1_{st}$ view mapping, which can thus be represented by $0100$ (leftmost bit is $0_{th}$ bit). Besides, it covers $0_{th}$ aggregate term $(COUNT(T))$ and $0_{th}$ and $1_{st}$ relational subgoals, which are represented by $100$ and $11$ respectively. The bit array representations for other view mappings are listed in Table \ref{Table: view mapping}. To compute covering sets, the view mapping combinations from the cross product of $\{S_1, S_2, S_3\}$ (denoted by $S_1 \times S_2 \times S_3$) are considered, which are constructed by applying bit OR operation to merge view mappings and the terms covered by them. For example, by referencing Table \ref{Table: view mapping}, the covering set $\{M_4, M_5\}$ can be constructed by unioning bit array $0100$ and $0010$ and the aggregate terms (relational subgoals resp.) jointly covered by them are computed by unioning $100$ and $011$ ($11$ and $11$).

% To compute covering sets, the cross product of $S_1$ and $S_2$ are computed first, which leads to some view mapping combinations (called {\em sub-covering sets thereafter}) that can cover both the aggregate terms. For instance, $\{M_3, M_5\}$ is one such sub-covering set. The construction of sub-covering sets (and covering sets ultimately) involves merging the view mappings and the aggregate terms and relational subgoals covered by them, which facilitate the following redundancy removal step. For example, $\{M_3,M_6\}$ (picked from $S_1$ and $S_2$ respectively) can jointly cover both the aggregate terms and relational subgoals of $Q$, which, however, is redundant with respect to $\{M_6\}$ (picked from $S_1$ and $S_2$ and one copy is retained) since $\{M_6\}$ is a {\em subset} of $\{M_3,M_6\}$ but still covers the same aggregate terms and relational subgoals as $\{M_3,M_6\}$. So it is safe to remove $\{M_3, M_6\}$ at this point. Then the resulting sub-covering sets are combined with the view mapping set in $S_3$ to compute the final covering sets.

% Note that two key operations are essential in this step, i.e. 1) merging view mappings and the terms covered by them; 2) removing redundancy by subset checking, which can be optimized by applying bit operations.  Three bit arrays are introduced for view mapping combinations, aggregate terms and relational subgoals, in which $i_th$ bit is 0 or 1 if $i_th$ view mapping/aggregate term/relational subgoal is included or missing in the sub-covering set. For example, for sub-covering set $\{M_3, M_6\}$, we use bit array 1001 to represent the view mapping in it (the leftmost 1 is in the 0th bit, which indicates the existence of $M_3$). Similarly, since they cover both the aggregate terms and relational subgoals, the other two bit arrays will be both 11. So when merging sub-covering set (or view mappings) with other sub-covering set (or view mappings), we can apply bit OR operations over the three bit arrays. Similarly, for subset checking operation, bit AND operation is applicable.



% \subsubsection{Applying clustering algorithms}
Since cross product operation ($\times$) is commutative and associative, different orders of operands result in the same output but different performance in the context of covering set computation. For example, in terms of $S_1\times S_2 \times S_3$, if $S_2 \times S_3$ is computed first, the result is $\{M_3, M_5\}$, $\{M_5, M_5\}$, $\{M_3, M_3\}$, $\{M_5, M_3\}$. After removing obvious redundancy, the result is rewritten as $\{M_3, M_5\}$, $\{M_5\}$, $\{M_3\}$. Note that $\{M_3, M_5\}$ is a duplicate compared to $\{M_3\}$ since 1) $\{M_3, M_5\}$ and $\{M_3\}$ cover the same set of aggregate terms and relational subgoals (checked by comparing the corresponding bit arrays); 2) $\{M_3\}$ is a subset of $\{M_3, M_5\}$. It's safe to remove $\{M_3, M_5\}$ at this point since in the final result, any view mapping combinations including $\{M_3, M_5\}$ will be a duplicate compared to that including $\{M_3\}$ and thus won't be a covering set. the intermediate result will be $\{\{M_3\}, \{M_4\}\}$, which is smaller than the result of the other option (merging $S_1$ and $S_2$ first), i.e. $\{\{M_3, M_5\}, \{M_4, M_5\}, \{M_6\}\}$. For the purpose of better merging strategy, clustering algorithms are applied such that the view mapping sets that are similar to each other can be clustered and thus merged first (e.g. $S_1$ and $S_3$). In ProCite, affinity propagation clustering algorithm \cite{dueck2007non} is used since it does not require pre-defined cluster number.

